{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "from sampling.Myspeculative_sampling import speculative_sampling\n",
    "from sampling.Myautoregressive_sampling import autoregressive_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig,AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_torch_init():\n",
    "    \"\"\"\n",
    "    Disable the redundant torch default initialization to accelerate model creation.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n",
    "    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name_from_path(model_path):\n",
    "    model_path = model_path.strip(\"/\")\n",
    "    model_paths = model_path.split(\"/\")\n",
    "    if model_paths[-1].startswith('checkpoint-'):\n",
    "        return model_paths[-2] + \"_\" + model_paths[-1]\n",
    "    else:\n",
    "        return model_paths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    disable_torch_init()\n",
    "\n",
    "    gamma=args.gamma\n",
    "\n",
    "    top_k = 1\n",
    "    top_p = 0.9\n",
    "    temperature=args.temperature\n",
    "\n",
    "    random_seed=args.seed\n",
    "\n",
    "    device_map=\"cuda:2\"\n",
    "    kwargs = {\"device_map\": device_map, **kwargs}\n",
    "\n",
    "    print('Loading  draft  model...')\n",
    "  \n",
    "    draft_model = Qwen2VLForConditionalGeneration.from_pretrained(args.draft_model_path,**kwargs)\n",
    "\n",
    "    print('Loading  target  model...')\n",
    "\n",
    "    target_model = Qwen2VLForConditionalGeneration.from_pretrained(args.target_model_path,**kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.target_model_path, use_fast=True)\n",
    "    processor = AutoProcessor.from_pretrained(args.target_model_path)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len=args.max_new_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--draft_model_path DRAFT_MODEL_PATH]\n",
      "                             [--target_model_path TARGET_MODEL_PATH]\n",
      "                             [--temperature TEMPERATURE]\n",
      "                             [--max_new_tokens MAX_NEW_TOKENS] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/bks/.local/share/jupyter/runtime/kernel-v38e9fbe4f33c9ad54c9e34a8a31d66f5a581a7fb1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='test')\n",
    "    parser.add_argument('--draft_model_path', type=str, default=\"/data/share/model_weight/qwen/Qwen2.5-0.5B\")\n",
    "    parser.add_argument('--target_model_path', type=str, default=\"/data/share/model_weight/qwen/Qwen2.5-7B\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=1)\n",
    "    parser.add_argument(\"--max_new_tokens\", type=int, default=30)\n",
    "    parser.add_argument('--seed', '-s', type=int, default=66, help='set a random seed, which can makes the result reproducible')\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TriForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
